{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Mount Google Drive & Load JSON Dataset**"
      ],
      "metadata": {
        "id": "HC1BDkTz7tXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vcPLDHuqh_HZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wpA7mQQkqki",
        "outputId": "a73e3794-bb2c-4ffa-9942-f88be5de46af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8c_zDlhkrJH",
        "outputId": "15549b1f-e421-4d50-c432-337960a9d592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample data: [('mdbl', 10996), ('fawc', 16260), ('degussa', 12089), ('woods', 8803), ('hanging', 13796), ('localized', 20672), ('sation', 20673), ('chanthaburi', 20675), ('refunding', 10997), ('hermann', 8804), ('passsengers', 20676), ('stipulate', 20677), ('heublein', 8352), ('screaming', 20713), ('tcby', 16261), ('four', 185), ('grains', 1642), ('broiler', 20680), ('wooden', 12090), ('wednesday', 1220), ('highveld', 13797), ('duffour', 7593), ('0053', 20681), ('elections', 3914), ('270', 2563), ('271', 3551), ('272', 5113), ('273', 3552), ('274', 3400), ('rudman', 7975), ('276', 3401), ('277', 3478), ('278', 3632), ('279', 4309), ('dormancy', 9381), ('errors', 7247), ('deferred', 3086), ('sptnd', 20683), ('cooking', 8805), ('stratabit', 20684), ('designing', 16262), ('metalurgicos', 20685), ('databank', 13798), ('300er', 20686), ('shocks', 20687), ('nawg', 7972), ('tnta', 20688), ('perforations', 20689), ('affiliates', 2891), ('27p', 20690), ('ching', 16263), ('china', 595), ('wagyu', 16264), ('affiliated', 3189), ('chino', 16265), ('chinh', 16266), ('slickline', 20692), ('doldrums', 13799), ('kids', 12092), ('climbed', 3028), ('controversy', 6693), ('kidd', 20693), ('spotty', 12093), ('rebel', 12639), ('millimetres', 9382), ('golden', 4007), ('projection', 5689), ('stern', 12094), (\"hudson's\", 7903), ('dna', 10066), ('dnc', 20695), ('hodler', 20696), ('lme', 2394), ('insolvancy', 20697), ('music', 13800), ('therefore', 1984), ('dns', 10998), ('distortions', 6959), ('thassos', 13801), ('populations', 20698), ('meteorologist', 8806), ('loss', 43), ('exco', 9383), ('adventist', 20813), ('murchison', 16267), ('locked', 10999), ('kampala', 13802), ('arndt', 20699), ('nakasone', 1267), ('steinweg', 20700), (\"india's\", 3633), ('wang', 3029), ('wane', 10067), ('unjust', 13803), ('titanium', 13804), ('want', 850), ('pinto', 20701), (\"institutes'\", 16268), ('absolute', 7973), ('travel', 4677), ('cutback', 6422), ('nazmi', 16269), ('modest', 1858), ('shopwell', 16270), ('sedi', 20702), ('adoped', 20703), ('tulis', 16271), ('18th', 20704), (\"wmc's\", 20705), ('menlo', 20706), ('reiners', 11000), ('farmlands', 12095), ('nonsensical', 20707), ('elisra', 20708), ('welcomed', 2461), ('peup', 20709), (\"holiday's\", 16272), ('activating', 20711), ('avondale', 16273), ('interational', 16274), ('welcomes', 20712), ('fip', 16275), ('tailings', 11001), ('fit', 4205), ('lifeline', 16276), ('bringing', 1916), ('fix', 4819), ('624', 6164), ('naturalite', 12096), ('wales', 6165), ('fin', 8807), ('fio', 11129), ('ceremenony', 20714), ('sovr', 20715), (\"yeo's\", 20716), ('effects', 1788), ('sixteen', 13805), ('undeveloped', 8808), ('glutted', 13806), ('barton', 20717), ('froday', 20718), ('arrow', 10089), ('stabilises', 11002), ('allan', 6960), ('374p', 20719), ('393', 3891), ('392', 4008), ('391', 4206), ('390', 3079), ('397', 4550), ('396', 6166), ('395', 6423), ('394', 4207), ('399', 6961), ('398', 4208), ('stabilised', 7595), ('smelters', 5114), ('oprah', 20720), ('orginially', 20721), (\"tvx's\", 20722), ('ponomarev', 16278), ('enviroment', 20723), (\"reeves'\", 20724), ('mason', 8363), ('encourage', 1670), ('adapt', 7596), ('abbott', 12776), ('stamping', 13808), ('colquiri', 20726), ('ambrit', 11003), ('strata', 8353), ('corrects', 4821), ('sandra', 11922), ('estimate', 859), ('universally', 20727), ('chlorine', 20728), ('competes', 16279), ('leiner', 10068), ('ministries', 8809), ('disturbed', 8810), ('competed', 13809), ('juergen', 8811), ('kfw', 13810), ('turben', 11004), ('reintroduced', 9384), ('maladies', 20729), ('chevron', 4101), ('lazere', 16280), ('antilles', 8812), ('dti', 11907), ('specially', 9070), ('bilzerian', 4678), ('bakelite', 13811), ('renovated', 20730), ('service', 568), ('payless', 16281), ('spiegler', 20731), ('needed', 831), ('wigglesworth', 16282), ('master', 6962), ('antonson', 13812), ('genesis', 20732), ('vismara', 13813), ('organically', 20734), (\"accords'\", 20735), ('task', 5940), ('positively', 7974), ('feasibility', 3479), ('ahmed', 6963), (\"suralco's\", 13814), ('awacs', 20736), ('idly', 16283), ('regulator', 20737), ('pseudorabies', 12097), ('staubli', 16284), ('nzi', 8813), ('feeling', 5115), ('275', 3127), ('6819', 20738), ('gorman', 16285), ('sustaining', 8354), ('spectrum', 9385), ('consenting', 20739), ('recapitalized', 12098), ('sailed', 11562), ('dozen', 7597), ('affairs', 1985), ('courier', 2253), ('kremlin', 8355), ('shipments', 895), (\"aquino's\", 16286), ('committing', 10070), ('sugarcane', 5293), ('diminishing', 9386), ('vexing', 16287), ('simplify', 11005), ('mouth', 6167), ('steinhardt', 7248), ('conceded', 8814), ('bradford', 9387), ('singer', 7976), ('5602', 20740), (\"1987's\", 13816), ('tech', 4950), ('teck', 6424), ('majv', 20741), ('saying', 666), ('dickey', 16477), ('sweetner', 20742), ('teresa', 21149), ('ulcer', 20743), ('cheaply', 13817), ('thai', 2361), ('orleans', 6964), ('excavator', 16290), ('rico', 6168), ('lube', 12099), ('rick', 13818), ('rich', 4679), ('kerna', 13819), ('rice', 950), ('rica', 4209), ('plate', 5503), ('platt', 16291), ('altogether', 8356), ('jaguar', 8815), ('dynair', 20744), ('patch', 8816), ('ldp', 2892), ('boarded', 13820), ('precluding', 16292), ('clarified', 11006), ('sensitivity', 16293), ('alternative', 1511), ('clarifies', 11007), ('lots', 5116), ('irs', 7598), ('irv', 20745), ('iri', 13821), ('ira', 13822), ('timber', 5690), ('ire', 20746), ('discipline', 5219), ('extend', 1937), ('nature', 3634), (\"amb's\", 16295), ('dunhill', 16296), ('extent', 2142), ('restrcitions', 20747), ('heating', 2396), (\"mannesmann's\", 11008), ('outsanding', 20748), ('multimillions', 20749), ('sarcinelli', 13824), ('southeastern', 6694), ('eradicate', 10071), ('libyan', 9388), ('foreclosing', 20750), ('maclaine', 12101), ('fra', 20751), ('union', 353), ('frn', 11009), ('much', 386), ('fry', 12102), ('mothball', 20752), ('chlorazepate', 10072), ('dxns', 12103), ('toyko', 19981), ('spit', 20753), ('007050', 16297), ('freehold', 16298), ('davy', 13825), ('dave', 11010), ('spie', 12177), ('aguayo', 10117), ('wildcat', 12104), ('fecs', 10069), ('kennan', 20754), ('intal', 16299), ('contingencies', 9389), ('professionally', 16551), ('microbiological', 16300), ('misconstrued', 20756), ('k', 409), ('securitiesd', 20757), ('deferring', 16301), ('kohl', 5941), ('conditioned', 3030), ('fnhb', 20758), (\"october's\", 16302), ('memorial', 13954), ('democracies', 6965), ('conformed', 27520), ('split', 464), (\"bond's\", 12105), ('thinly', 11112), ('dunkirk', 16515), ('cavanaugh', 16303), (\"securities'\", 13827), ('marches', 21345), ('issam', 16304), ('workforce', 2020), ('meinert', 12106), ('boiler', 13828), (\"bp's\", 5294), ('torpedoed', 16305), ('indidate', 20762), ('downwardly', 13829), ('viviez', 20763), ('vladiminovich', 20764), ('academic', 16306), ('architecural', 20765), ('corporate', 1117), ('appropriately', 16307), ('teicc', 20766), (\"hanover's\", 20767), ('aristech', 8817), ('portrayed', 20768), ('raffineries', 21383), ('hai', 20770), ('hal', 7599), ('ham', 13830), ('han', 10073), ('e15b', 20771), ('had', 61), ('hay', 20772), ('botchwey', 13831), ('haq', 10074), ('has', 37), ('hat', 13832), ('hav', 20773), ('fortin', 20774), ('municipal', 8818), ('osman', 20775), ('fsical', 20776), ('elders', 3480), ('survival', 12107), ('unequivocally', 16308), ('objective', 2519), ('indicative', 6695), ('shadow', 10075), ('riskiness', 21411), ('positiive', 20778), (\"american's\", 10076), ('alick', 16309), ('harima', 16310), ('alice', 12108), ('altschul', 20779), ('festivities', 16311), ('medecines', 20780), ('beneficial', 2942), ('yoweri', 12109), ('crowd', 13833), ('crowe', 9390), ('crown', 3553), ('topping', 13679), ('captive', 8819), ('billboard', 12110), ('fiduciary', 6169), ('bottom', 3402), ('plucked', 20782), ('locksmithing', 20783), ('ecopetrol', 9391), ('pipestone', 24018), (\"growers'\", 5505), ('borrows', 20785), ('eduard', 16312), ('venpres', 13834), ('bamboo', 16313), ('foolish', 13835), ('uruguyan', 20786), ('officeholders', 20787), ('economiques', 20788), ('aden', 16314), ('maxwell', 4822), ('marshall', 4680), ('honeymoon', 16315), ('administer', 16316), ('shoots', 20790), ('rubbertech', 16317), ('johsen', 16318), ('reciprocity', 10077), ('fabric', 13836), ('suffice', 20791), ('spokemsan', 20792), (\"sonora's\", 20793), ('5865', 16319), (\"systems'\", 16320), ('perfumes', 20794), ('halycon', 20795), ('nonvoting', 20796), ('safeguard', 7250), ('sawdust', 21538), (\"else's\", 20797), ('arrays', 13837), ('aza', 20798), ('smasher', 20799), ('complications', 12111), ('pesos', 1813), ('relabelling', 20800), ('passenger', 3722), (\"avon's\", 12112), ('megahertz', 20801), ('mirror', 10683), ('minas', 8357), ('bourdain', 16322), ('crownx', 20802), ('eventual', 6425), ('crowns', 1207), ('role', 1369), ('obliges', 20803), ('rolf', 16323), ('vegetative', 13838), ('rolm', 20804), ('roll', 4419), ('intend', 2463), ('palms', 16324), ('denys', 19255), ('transported', 13839), ('moresby', 20805), ('devon', 16325), ('intent', 1351), (\"camco's\", 20806), ('variable', 5942), ('transporter', 20807), ('danske', 16326), ('friedhelm', 13840), ('hawker', 8358), (\"sand's\", 17774), ('preseving', 20808), ('80386', 12113), ('bnls', 16328), ('ordination', 19984), ('overturned', 11011), ('erred', 16329), ('cincinnati', 6696), ('corps', 16710), ('whoever', 20809), ('osp', 16330), ('osr', 13841), ('ost', 12114), ('chair', 16331), ('690', 5647), ('grapples', 20810), ('megawatts', 13842), ('photocopiers', 20811), ('sconninx', 20812), ('circumstances', 2274), ('oversight', 13843), (\"paradyne's\", 20814), ('691', 6363), ('paychecks', 20815), (\"stadelmann's\", 13844), ('choice', 3241), ('vastagh', 11012), ('embark', 8820), ('gloomy', 9392), ('stays', 9393), ('exact', 4009), ('minute', 5117)]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for Google Drive and file handling\n",
        "import json\n",
        "\n",
        "# Specify the path to your JSON file on Google Drive\n",
        "file_path = '/content/drive/MyDrive/Data_Analysis_using_python/reuters_word_index.json'  # Replace with your actual file path\n",
        "\n",
        "# Load the JSON dataset\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Display the first few entries to verify loading\n",
        "print(\"Sample data:\", list(data.items())[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Preprocess Vocabulary (Tokenize & Clean Words)**"
      ],
      "metadata": {
        "id": "1Pf6l9tc9w86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Text cleaning and tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "words = list(data.keys())\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\b[a-zA-Z]{2,}\\b')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(word):\n",
        "    word = word.lower()\n",
        "    return [t for t in tokenizer.tokenize(word) if t not in stop_words]\n",
        "\n",
        "tokens = [preprocess(w) for w in words]\n",
        "tokens = [t for sub in tokens for t in sub]  # Flatten\n",
        "print(\"Clean tokens:\", tokens[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0bOLqZ_U50u",
        "outputId": "c729b19a-888d-4baf-8782-1bd8940556f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean tokens: ['mdbl', 'fawc', 'degussa', 'woods', 'hanging', 'localized', 'sation', 'chanthaburi', 'refunding', 'hermann']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Install Gensim & Fix Compatibility**"
      ],
      "metadata": {
        "id": "l79YdUGB92Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNYd80shVLrJ",
        "outputId": "16e78f38-3c0b-4c5f-c4df-9a4aa7b34539"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.24.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 💥 Force fresh installs\n",
        "!pip uninstall -y numpy scipy gensim\n",
        "!pip install -U numpy==1.24.3 scipy==1.11.3 gensim==4.3.1 --force-reinstall --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbZOuvl-VQAL",
        "outputId": "9952aa67-ff4f-4dd1-a369-4490114a1a2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scipy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.3 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Train Word2Vec Model on Cleaned Tokens**"
      ],
      "metadata": {
        "id": "haDJhEWw96gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "print(\"Gensim import successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uusiB9fF_R_E",
        "outputId": "9a0f1c02-64ad-4084-fb67-347832687479"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gensim import successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train Word2Vec on single-token sentences\n",
        "sentences = [[token] for token in tokens]\n",
        "model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"reuters_word2vec.model\")\n",
        "\n",
        "# Show vector for a word\n",
        "sample_word = \"china\"\n",
        "if sample_word in model.wv:\n",
        "    print(f\"Vector for '{sample_word}':\", model.wv[sample_word][:10])\n",
        "else:\n",
        "    print(f\"'{sample_word}' not in vocabulary.\")"
      ],
      "metadata": {
        "id": "nuJkz9J7U8zS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e4d193b-b232-4e61-ddec-8af39c2737af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'china': [ 7.1148876e-05  4.1307653e-03  5.5061770e-03  4.2398069e-03\n",
            " -6.1513605e-03  6.8574082e-03 -3.8503288e-04  2.7118577e-03\n",
            "  6.7851674e-03  9.3285581e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Convert Tokens to Sequences & Prepare Labels**"
      ],
      "metadata": {
        "id": "YAIiZ3519-QA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Build vocabulary index\n",
        "vocab = {word: idx + 1 for idx, word in enumerate(set(tokens))}\n",
        "vocab_size = len(vocab) + 1\n",
        "\n",
        "# Convert words into 1D integer array (since you're using 1-word sequences)\n",
        "indexed_tokens = np.array([vocab[word] for word in tokens]).reshape(-1, 1)\n",
        "\n",
        "# Create synthetic binary labels (0 or 1)\n",
        "labels = np.random.randint(0, 2, size=(len(indexed_tokens), ))\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(indexed_tokens, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "D_oPqldrVkXO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Build Embedding Matrix from Word2Vec Vectors**"
      ],
      "metadata": {
        "id": "CRfVeAuE-CUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, idx in vocab.items():\n",
        "    if word in model.wv:\n",
        "        embedding_matrix[idx] = model.wv[word]"
      ],
      "metadata": {
        "id": "PrPUQrRNV_e_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Install PyTorch & Prepare Datasets**"
      ],
      "metadata": {
        "id": "bbK2QUiL-Frd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --quiet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "aAsqdCxgWbip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7af7ca-2ba7-4058-d224-7f1ccaf125a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1)\n",
        "\n",
        "# Create loaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "2N1Z7C1zWoTI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Define & Train PyTorch Neural Network Model**"
      ],
      "metadata": {
        "id": "QLlOFje--KCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(self.relu(self.fc1(x)))\n",
        "        return self.sigmoid(self.fc2(x))\n",
        "\n",
        "model = SimpleNet()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Suw0JD4SWpwA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/10 - Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "HV0JPnszWrrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b52296-13dc-443a-edb5-965af29156e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 47.3684\n",
            "Epoch 2/10 - Loss: 57.8947\n",
            "Epoch 3/10 - Loss: 73.6842\n",
            "Epoch 4/10 - Loss: 52.6316\n",
            "Epoch 5/10 - Loss: 52.6316\n",
            "Epoch 6/10 - Loss: 63.1579\n",
            "Epoch 7/10 - Loss: 36.8421\n",
            "Epoch 8/10 - Loss: 31.5789\n",
            "Epoch 9/10 - Loss: 47.3684\n",
            "Epoch 10/10 - Loss: 52.6316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluate Model Accuracy**"
      ],
      "metadata": {
        "id": "Xc8JT_zR-Q2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        predictions = model(batch_X).round()\n",
        "        correct += (predictions == batch_y).sum().item()\n",
        "        total += batch_y.size(0)\n",
        "\n",
        "print(f\"\\n✅ PyTorch Test Accuracy: {correct / total:.4f}\")"
      ],
      "metadata": {
        "id": "BMU7WodoWs8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f0b70a0-af1d-4e7c-9939-fb0af9ace8a2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ PyTorch Test Accuracy: 0.4992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Check Prediction on New Input Token**\n",
        "\n"
      ],
      "metadata": {
        "id": "t_s9NZcUZB0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a word\n",
        "word = \"china\"\n",
        "\n",
        "# Check if it exists in vocab\n",
        "if word in vocab:\n",
        "    word_index = torch.FloatTensor([[vocab[word]]])  # shape [1, 1]\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(word_index)\n",
        "        predicted_class = int(prediction.round().item())\n",
        "        print(f\"Input word: '{word}'\")\n",
        "        print(f\"Predicted Class: {predicted_class}\")\n",
        "else:\n",
        "    print(f\"'{word}' not in vocabulary.\")"
      ],
      "metadata": {
        "id": "i-wAgbnnYCb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c55004d-a9f6-4484-be03-33d5d3f0dd95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input word: 'china'\n",
            "Predicted Class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Try Multiple Words at Once**"
      ],
      "metadata": {
        "id": "z4l7fejkZFcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_words = [\"oil\", \"bank\", \"market\", \"trade\", \"currency\"]\n",
        "\n",
        "model.eval()\n",
        "for word in test_words:\n",
        "    if word in vocab:\n",
        "        x = torch.FloatTensor([[vocab[word]]])\n",
        "        with torch.no_grad():\n",
        "            y_pred = model(x).item()\n",
        "            print(f\"{word:>10} → Confidence: {y_pred:.4f} → Class: {int(round(y_pred))}\")\n",
        "    else:\n",
        "        print(f\"{word:>10} → Not in vocab\")"
      ],
      "metadata": {
        "id": "kwSHjcijYlIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27ffde8-5e20-4872-a62b-f3732d5ff518"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       oil → Confidence: 1.0000 → Class: 1\n",
            "      bank → Confidence: 1.0000 → Class: 1\n",
            "    market → Confidence: 1.0000 → Class: 1\n",
            "     trade → Confidence: 1.0000 → Class: 1\n",
            "  currency → Confidence: 1.0000 → Class: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Test the Word2Vec Model**"
      ],
      "metadata": {
        "id": "RGrowGNZZK_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After Word2Vec training\n",
        "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.save(\"reuters_word2vec.model\")\n",
        "\n",
        "\n",
        "\n",
        "# Word similarity test\n",
        "if \"oil\" in word2vec_model.wv:\n",
        "    similar = word2vec_model.wv.most_similar(\"oil\", topn=5)\n",
        "    print(\"Words most similar to 'oil':\")\n",
        "    for word, score in similar:\n",
        "        print(f\"{word}: {score:.4f}\")\n",
        "else:\n",
        "    print(\"'oil' not found in Word2Vec vocabulary.\")"
      ],
      "metadata": {
        "id": "ULkmo9caYlf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc678594-6e25-4403-9298-73fe72f03e6e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'oil':\n",
            "hubert: 0.4006\n",
            "onto: 0.3917\n",
            "manly: 0.3883\n",
            "wild: 0.3862\n",
            "westcoast: 0.3819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Visual Check: Accuracy Was Actually Trained**"
      ],
      "metadata": {
        "id": "-_w_8d4QZOag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final model test accuracy: {correct / total:.4f}\")"
      ],
      "metadata": {
        "id": "-tNFFnggYnWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "005ea6ba-532a-4b7b-9c1e-7744c003ce05"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final model test accuracy: 0.4992\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t_s9NZcUZB0_",
        "z4l7fejkZFcz",
        "RGrowGNZZK_x",
        "-_w_8d4QZOag"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}